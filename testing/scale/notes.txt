pip install locustio

Use the web ui to drive - set up the stacks using the partition key
set up, then verify the inserts done in the east show up in the
west.

locust -f simple.py

aws dynamodb scan --table-name PKTestTable --select "COUNT"
aws dynamodb scan --table-name PKTestTable --select "COUNT" --region us-west-2


Observations

20171006

Python version - eliminated call to describe_table to get hash attribute name, which
increase scale

Write cap: 20
Read cap: 10
Memory size: 512
Batch size: 20
Timeout: 20

See notes below to get a read on write/s associated with number of clients

15 clients

east/west
250/258
500/508
750/756

20 clients

east/west
500/505
750/757
1000/1006

30 clients

east/west
500/448
1000/910
1500/1373

20171006

Node version much better but I noticed a potential perf inhibitor in the 
python impl which will be addressed later.

Also note instead of put item we should try batch write item so stay tuned for that.

Write cap: 20
Read cap: 10
Memory size: 512
Batch size: 20
Timeout: 20

Node 5 users 1.6 TPS
east/west
250/253
500/500
...

Node 10 users 3.21 TPS
east/west
250/255
500/507

Node 15 users 4.8 TPS
east/west
250/260
500/507

Node 20 users
east/west
500/506
1000/1003
1500/1508

Node 25 users 7.9 TPS
east/west
500/504
1000/1010
1500/1508

Node 50 users 16.7 TPS
east/west
500/525
1000/1012
1500/1522
2000/2028

Node 70 users 22 TPS - note: went over write cap should be sustainable but reset ddb capacity
east/west
500/526
1000/1038
1500/1531
2000/2079


20171005

Read cap 5, write cap 10

Inserts us-east     Records west

1000                679
2000                1288
3000                2003
4000                2655
5000                3371

+30s                3536 (inserts us-east stopped)
+60s                3711
+2m                 4061
+3m                 4738
+4m                 5055

Lambda - batch size 25, mem 512, timeout 20s

Write were ~ 8.3 TPS (25 locust clients), replication ~ 5 TPS

Double write cap on the DDB tables did not alter stream throughput

Set cap to 10 write/5 read

13 client - here the replication keeps up with writes at 4.1 TPS. West count is higher as I had to change
windows and execute a scan for the count

east west 
500 519
1000 1019
1500 1510

16 clients - replication keeps up, writes at 5.15 TPS

500/515
1000/1006
1500/1509

20 clients - write tps ~ 6.25 TPS - can't keep up

500/261
1000/644
1500/998

Boost write cap to 20, read at 5

20 clients - write tps ~ 6.25 - again we see the replication bottle necked at
~ 5.5 TPS

500/414
1000/817
1500/1250

Can increasing the lambda resources change this? Same as above but go from
512 MB to 1024 (20 clients ~6.25 writes/s)

500/411
1000/808
1500/1221

Single threaded lambda probably the bottle neck - next up: try node impl with some concurrency

20171004

error modes - function timeout before processing all the records ->
function gets called again with same set of records

25 locust users, did aboout 10,000 inserts

1st check in on east:

{
    "Count": 10107, 
    "ScannedCount": 10107, 
    "ConsumedCapacity": null
}

West was at:

{
    "Count": 5543, 
    "ScannedCount": 5543, 
    "ConsumedCapacity": null
}

The current approach is bottle necked - need to find a way to introduce some concurrency.